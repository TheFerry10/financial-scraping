{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import argparse\n",
    "\n",
    "# Argument parsing\n",
    "parser = argparse.ArgumentParser(description='Extract article meta data for all articles related to a stock in a specified time range.')\n",
    "parser.add_argument('ticker', type='string', help='stock ticker')\n",
    "parser.add_argument('start', type='string', help='start date YYYY-MM-DD')\n",
    "parser.add_argument('end', type='string', help='end date YYYY-MM-DD')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Input data\n",
    "start_date = date.fromisoformat(args['start'])\n",
    "end_date = date.fromisoformat(args['end'])\n",
    "stock_ticker = [t.strip() for t in args['ticker'].upper().split(',')]\n",
    "\n",
    "print(\"Ticker: \", stock_ticker)\n",
    "print(\"Start date: \", str(start_date))\n",
    "print(\"End date: \", str(end_date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "df_stocks = pd.read_csv(\"data/stocks/processed/20220824T074737_xetra_finanzen.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_from_string(string):\n",
    "    result = hashlib.sha1(string.encode())\n",
    "    return result.hexdigest()\n",
    "\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "def get_links_for_all_pages(soup):\n",
    "    pagination_list_object = soup.find('ul', {'class': \"pagination__list\"})\n",
    "    if pagination_list_object:\n",
    "        pagination_objects = pagination_list_object.find_all(\n",
    "            'a', {'class': 'pagination__text'})\n",
    "        links = [pagination.attrs['href'] for pagination in pagination_objects][:-1]\n",
    "        return links\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def get_news_links(df_stocks, start_date, end_date):\n",
    "    base_url = \"https://www.finanzen.net\"\n",
    "    extracted_news_properties = []\n",
    "    failed_links = []\n",
    "    for ISIN, url_news in df_stocks[['ISIN', 'news_link']].values:\n",
    "        links_all_pages = get_links_for_all_pages(get_soup(url_news))\n",
    "        if links_all_pages:\n",
    "            for link in links_all_pages:\n",
    "                url = base_url + link\n",
    "                print(url)\n",
    "                soup = get_soup(url)\n",
    "                for news in soup.find_all('div', {'class': 'news news--item-with-media'}):\n",
    "                    news_entry = dict()\n",
    "                    date = news.find('time', {'class': 'news__date'})\n",
    "                    article_date = datetime.strptime(date.text, '%d.%m.%y').date()\n",
    "                    if (start_date <= article_date <= end_date):\n",
    "                        source = news.find('span', {'class': 'news__source'})\n",
    "                        kicker = news.find('span', {'class': 'news__kicker'})\n",
    "                        title = news.find('span', {'class': 'news__title'})\n",
    "                        link = news.find('a', {'class': 'news__card'}).attrs['href']\n",
    "                        id = get_hash_from_string(link)\n",
    "                        keys = ['id', 'ISIN', 'date', 'title', 'source', 'kicker', 'link_article']\n",
    "                        values = [id, ISIN, date, title, source, kicker, link]\n",
    "                        for key, value in zip(keys, values):\n",
    "                            if hasattr(value, 'text'):\n",
    "                                news_entry[key] = value.text.encode('latin').decode()\n",
    "                            else:\n",
    "                                news_entry[key] = value\n",
    "                        if news_entry['link_article']:\n",
    "                            news_entry['link_article'] = base_url + \\\n",
    "                                news_entry['link_article']\n",
    "\n",
    "                        extracted_news_properties.append(news_entry)\n",
    "        else:\n",
    "            print(url_news)\n",
    "            failed_links.append(url_news)\n",
    "    return extracted_news_properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.finanzen.net/news/unilever-news\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_2\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_3\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_4\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_5\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_6\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_7\n"
     ]
    }
   ],
   "source": [
    "df_stocks_selected = df_stocks.query(\"Symbol in @stock_ticker\").copy()\n",
    "extracted_news_properties = get_news_links(df_stocks_selected, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tickers = '-'.join(stock_ticker)\n",
    "file_name = '_'.join(['news', stock_tickers, str(start_date), str(end_date)])\n",
    "file_name += '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to data/stocks/news_UNLYF_2021-01-01_2021-12-31.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"data/stocks/{file_name}\"\n",
    "df_stock_news = pd.DataFrame(extracted_news_properties)\n",
    "df_stock_news.to_csv(file_path, index=False)\n",
    "print(f\"Save to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3577d3b9980a21cfb532b7187d54a40e95e0cbfb62931e8c4ec2f5fbb4bbb23"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('financial_scraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
