{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_from_string(string):\n",
    "    result = hashlib.sha1(string.encode())\n",
    "    return result.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "base_url = \"https://www.finanzen.net\"\n",
    "df_stocks = pd.read_csv(\"data/stocks/processed/20220824T074737_xetra_finanzen.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "stock_ticker = ['UNLYF']\n",
    "start_date = date(2021,1,1)\n",
    "end_date = date(2021,12,31)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_for_all_pages(soup):\n",
    "    pagination_list_object = soup.find('ul', {'class': \"pagination__list\"})\n",
    "    if pagination_list_object:\n",
    "        pagination_objects = pagination_list_object.find_all(\n",
    "            'a', {'class': 'pagination__text'})\n",
    "        links = [pagination.attrs['href'] for pagination in pagination_objects][:-1]\n",
    "        return links\n",
    "    else:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(df_stocks, start_date, end_date):\n",
    "    extracted_news_properties = []\n",
    "    failed_links = []\n",
    "    for ISIN, url_news in df_stocks[['ISIN', 'news_link']].values:\n",
    "        links_all_pages = get_links_for_all_pages(get_soup(url_news))\n",
    "        if links_all_pages:\n",
    "            for link in links_all_pages:\n",
    "                url = base_url + link\n",
    "                print(url)\n",
    "                soup = get_soup(url)\n",
    "                for news in soup.find_all('div', {'class': 'news news--item-with-media'}):\n",
    "                    news_entry = dict()\n",
    "                    date = news.find('time', {'class': 'news__date'})\n",
    "                    article_date = datetime.strptime(date.text, '%d.%m.%y').date()\n",
    "                    if (start_date <= article_date <= end_date):\n",
    "                        source = news.find('span', {'class': 'news__source'})\n",
    "                        kicker = news.find('span', {'class': 'news__kicker'})\n",
    "                        title = news.find('span', {'class': 'news__title'})\n",
    "                        link = news.find('a', {'class': 'news__card'}).attrs['href']\n",
    "                        id = get_hash_from_string(link)\n",
    "                        keys = ['id', 'ISIN', 'date', 'title', 'source', 'kicker', 'link_article']\n",
    "                        values = [id, ISIN, date, title, source, kicker, link]\n",
    "                        for key, value in zip(keys, values):\n",
    "                            if hasattr(value, 'text'):\n",
    "                                news_entry[key] = value.text.encode('latin').decode()\n",
    "                            else:\n",
    "                                news_entry[key] = value\n",
    "                        if news_entry['link_article']:\n",
    "                            news_entry['link_article'] = base_url + \\\n",
    "                                news_entry['link_article']\n",
    "\n",
    "                        extracted_news_properties.append(news_entry)\n",
    "        else:\n",
    "            print(url_news)\n",
    "            failed_links.append(url_news)\n",
    "    return extracted_news_properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.finanzen.net/news/unilever-news\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_2\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_3\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_4\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_5\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_6\n",
      "https://www.finanzen.net/news/unilever-news@intpagenr_7\n"
     ]
    }
   ],
   "source": [
    "df_stocks_selected = df_stocks.query(\"Symbol in @stock_ticker\").copy()\n",
    "extracted_news_properties = get_news_links(df_stocks_selected, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tickers = '-'.join(stock_ticker)\n",
    "file_name = '_'.join(['news', stock_tickers, str(start_date), str(end_date)])\n",
    "file_name += '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to data/stocks/news_UNLYF_2021-01-01_2021-12-31.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"data/stocks/{file_name}\"\n",
    "df_stock_news = pd.DataFrame(extracted_news_properties)\n",
    "df_stock_news.to_csv(file_path, index=False)\n",
    "print(f\"Save to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3577d3b9980a21cfb532b7187d54a40e95e0cbfb62931e8c4ec2f5fbb4bbb23"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('financial_scraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
