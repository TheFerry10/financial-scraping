{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the article tags for 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"news.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT timestamp, tags FROM Tagesschau\")\n",
    "results = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.DataFrame(results, columns=['timestamp', 'tags'])\n",
    "df_news['timestamp'] = pd.to_datetime(df_news['timestamp'])\n",
    "df_news.sort_values(\"timestamp\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple exploratory data analysis on tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article frequency\n",
    "* How many articles are published in 2022?\n",
    "* How are the articles distributed throughout the years?\n",
    "* Are there months where more articles are published than usual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_per_year = df_news.groupby(pd.Grouper(key=\"timestamp\", freq=\"1Y\")).agg({'tags': 'count'}).rename(columns={'tags': 'number of articles per year'})\n",
    "df_articles_per_month = df_news.groupby(pd.Grouper(key=\"timestamp\", freq=\"1M\")).agg({'tags': 'count'}).rename(columns={'tags': 'number of articles per month'})\n",
    "df_articles_per_week = df_news.groupby(pd.Grouper(key=\"timestamp\", freq=\"1W\")).agg({'tags': 'count'}).rename(columns={'tags': 'number of articles per week'})\n",
    "\n",
    "print(df_articles_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8), nrows=2, ncols=1, sharex=True)\n",
    "# articles per month\n",
    "sns.lineplot(\n",
    "    data=df_articles_per_month, color='C0', ax=ax[0])\n",
    "ax[0].hlines(y=df_articles_per_month.mean(), xmin=datetime(2022,1,1), xmax=datetime(2022,12,31), colors='C1', label='Average number of published articles', linestyles='dashed')\n",
    "ax[0].legend()\n",
    "\n",
    "# articles per week\n",
    "sns.lineplot(\n",
    "    data=df_articles_per_week, color='C0', ax=ax[1])\n",
    "ax[1].hlines(y=df_articles_per_week.mean(), xmin=datetime(2022,1,1), xmax=datetime(2022,12,31), colors='C1', label='Average number of published articles', linestyles='dashed')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tags\n",
    "* What are the unique tags used in 2022?\n",
    "* How many unique tags are used in 2022?\n",
    "* What are the top tags used in 2022?\n",
    "* What are the top tags used per month?\n",
    "* How is the frequency of the top tags distributed over the year?\n",
    "* How many tags are used per article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['tags_splitted'] = df_news['tags'].apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['tags_splitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tags are used per article?\n",
    "df_news['num_tags'] = df_news['tags_splitted'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for number of tags per article\n",
    "fig, ax = plt.subplots()\n",
    "sns.countplot(data=df_news, x='num_tags', color='C0', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_tags = []\n",
    "for tags in df_news['tags_splitted']:\n",
    "    all_tags.extend(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the unique tags used in 2022?\n",
    "unique_tags = sorted(list(set(all_tags)))\n",
    "print(\"The first 10 tags: \", unique_tags[:11])\n",
    "\n",
    "# How many unique tags are used in 2022?\n",
    "print(f\"The number of unique tags in 2022: {len(unique_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the top tags used in 2022?\n",
    "from collections import Counter\n",
    "# https://note.nkmk.me/en/python-collections-counter/\n",
    "tags_count_per_year = Counter(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the top tags used per month?\n",
    "df_tags_per_month = df_news.groupby(pd.Grouper(key=\"timestamp\", freq=\"1M\")).agg({'tags_splitted': 'sum'}).rename(columns={'tags_splitted': 'all tags'})\n",
    "df_tags_per_month['tags frequency'] = df_tags_per_month['all tags'].apply(Counter)\n",
    "top_n = 10\n",
    "df_tags_per_month['most frequently used tags'] = df_tags_per_month['tags frequency'].apply(lambda c: c.most_common(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How are the top tags distributed over the year?\n",
    "sns.barplot(\n",
    "    data=pd.DataFrame(df_tags_per_month.iloc[11][\"most frequently used tags\"], columns=['tag', 'count']),\n",
    "    y='tag',\n",
    "    x='count',\n",
    "    color='C0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_overall = 20\n",
    "df_top_n_tags = pd.DataFrame(tags_count_per_year.most_common(top_n_overall), columns=['tag', 'count'])\n",
    "sns.barplot(\n",
    "    data=df_top_n_tags,\n",
    "    y='tag',\n",
    "    x='count',\n",
    "    color='C0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal distribution of tag occurrence \n",
    "df_tags_occurrence = pd.DataFrame()\n",
    "for tag in df_top_n_tags['tag']:\n",
    "    df_tags_occurrence = pd.concat([\n",
    "        df_tags_occurrence,\n",
    "        df_news\\\n",
    "            .set_index('timestamp')['tags_splitted']\\\n",
    "            .apply(lambda x: 1 if tag in x else 0)\\\n",
    "            .groupby(pd.Grouper(freq=\"1W\"))\\\n",
    "            .sum()\\\n",
    "            .rename(tag)\n",
    "        ],axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_rel_occurrence = df_tags_occurrence / df_tags_occurrence.sum()\n",
    "sns.lineplot(data=df_tags_rel_occurrence[[\"Coronavirus\", \"Energiekrise\", \"Ukraine-Krieg\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\n",
    "df_tag_correlation_pearson = df_tags_occurrence.corr(method='pearson')\n",
    "df_tag_correlation_spearman = df_tags_occurrence.corr(method='spearman')\n",
    "\n",
    "def plot_correlation(df_tag_correlation, method):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    mask = np.triu(np.ones_like(df_tag_correlation, dtype=bool))\n",
    "    heatmap = sns.heatmap(df_tag_correlation, vmin=-1, vmax=1, annot=True, cmap='RdBu', mask=mask, cbar=False, fmt='.2f')\n",
    "    heatmap.set_title(f'{method.title()} correlation heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(df_tag_correlation_pearson, method='pearson')\n",
    "plot_correlation(df_tag_correlation_spearman, method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.statology.org/numpy-get-indices-where-true/\n",
    "# https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "def get_correlation_pairs(df_tag_correlation):\n",
    "    no_duplicated_tag_correlation = (df_tag_correlation * (np.triu(np.ones_like(df_tag_correlation, dtype=int)) - np.eye(len(df_tag_correlation), dtype=int)))\n",
    "    return no_duplicated_tag_correlation.unstack().sort_values(ascending=False).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_pairs_pearson = get_correlation_pairs(df_tag_correlation_pearson)\n",
    "correlation_pairs_spearman = get_correlation_pairs(df_tag_correlation_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_pairs_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_pairs_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_analyzer(comma_separated_tags):\n",
    "    return [x.split(',') for x in comma_separated_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = df_news['tags'].apply(lambda x: x.split(',')).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for all tags used and assign integers to each tag\n",
    "# all tags\n",
    "all_tags = []\n",
    "for tags_in_article in data_samples:\n",
    "    all_tags.extend(tags_in_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the top tags used?\n",
    "\n",
    "top_tags = df_tags_counts['tag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = sorted(set(top_tags))\n",
    "vocabulary = {key: value for key, value in zip(unique_tags, range(len(unique_tags)))}\n",
    "\n",
    "tag_to_index = {key: value for key, value in zip(unique_tags, range(len(unique_tags)))}\n",
    "index_to_tag = {key: value for key, value in enumerate(unique_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the article tags with the vocabulary mapping\n",
    "# example:\n",
    "# [['Corona', 'Liveblog'], ['Frankreich', 'Corona']] --> [[1, 2], [3, 1]]\n",
    "\n",
    "test_input = ['Corona', 'Liveblog', 'Test']\n",
    "test_output = [1, 2]\n",
    "test_vocabulary = {'Corona': 1, 'Liveblog': 2, 'Frankreich': '3'}\n",
    "\n",
    "def vectorize_tags(article_tags, vocabulary):\n",
    "    return [vocabulary[tag] for tag in article_tags if tag in vocabulary]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vectorize_tags(test_input, test_vocabulary) == test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_tags = [vectorize_tags(article_tags, tag_to_index) for article_tags in data_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = [[1, 2, 4], [2, 3, 1], [5, 4, 1]]\n",
    "test_vocabulary_id = [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1, 2, 4] -> [0, 1, 1, 0, 1, 0]\n",
    "# [2, 3, 1] -> [0, 1, 1, 1, 0, 0]\n",
    "\n",
    "# 1. get first id in doc\n",
    "# 2. increment position with 1 in target vector\n",
    "# 3. get next id in doc and do 2. again\n",
    "\n",
    "def count_vectorize(doc_ids, vocabulary):\n",
    "    num_docs = len(doc_ids)\n",
    "    num_vocabulary_entities = len(vocabulary.values())\n",
    "    vectors = np.zeros((num_docs, num_vocabulary_entities), dtype=int)\n",
    "    for doc_count, doc in enumerate((doc_ids)):\n",
    "        for id in doc:\n",
    "            vectors[doc_count, id] += 1\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_test = [[1, 2], [2, 3], [0], []]\n",
    "vocabulary_test = {'a':0, 'b':1, 'c':2, 'd':3}\n",
    "\n",
    "count_vectorize(doc_ids_test, vocabulary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorized = count_vectorize(vectorized_tags, tag_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tfidf results\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorized = tfidf.fit_transform(count_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=10)\n",
    "tsvd.fit(tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsvd = tsvd.transform(tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, max_iter=100, n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X_tsvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[index_to_tag[index] for index in tsvd.inverse_transform(kmeans.cluster_centers_).argsort()[:, ::-1][4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation()\n",
    "lda.fit(count_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.transform(vectorized_tfidf[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = lda.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = topic.argsort()[:-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[index_to_tag[index] for index in top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What tags are occurring together?\n",
    "\n",
    "tag = 'Russland'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give me all articles where I can find the tag\n",
    "relevant_tags = []\n",
    "for data_sample_ in data_samples:\n",
    "    if tag in data_sample_:\n",
    "        relevant_tags.append(data_sample_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant_tags_counts = pd.Series(flatten(relevant_tags)).value_counts().sort_values(ascending=False).reset_index()\n",
    "df_relevant_tags_counts.columns = ['tag', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant_tags_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.simple_preprocess(\"This is a simple line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=data_samples, window=2, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['EU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags_series.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.wmdistance([\"USA\"], [\"Homeoffice\"]))\n",
    "print(model.wv.wmdistance([\"USA\"], [\"Biden\"]))\n",
    "print(model.wv.wmdistance([\"USA\"], [\"Ukraine\"]))\n",
    "print(model.wv.wmdistance([\"Krieg\"], [\"Ukraine\"]))\n",
    "print(model.wv.wmdistance([\"Krieg\"], [\"Mondmission\"]))\n",
    "print(model.wv.wmdistance([\"Russland\"], [\"Putin\"]))\n",
    "print(model.wv.wmdistance([\"Impfung\"], [\"Putin\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar([\"Landtagswahl\"], topn=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "187b1988031df14b784b1ca83dcf6c508f75d9af158ade7af96a8719ac4cf2b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
